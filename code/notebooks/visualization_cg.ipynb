{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\n",
    "    'DeepSeek Coder 1.3B',\n",
    "    'DeepSeek Coder 6.7B',\n",
    "    'DeepSeek Coder 33B',\n",
    "    'CodeLlama 7B',\n",
    "    'CodeLlama 13B',\n",
    "    'CodeLlama 34B',\n",
    "    'GPT-3.5-turbo',\n",
    "    'GPT-4-turbo'\n",
    "]\n",
    "\n",
    "MODEL_NAMES_SHORT = [\n",
    "    'DSC 1.3B',\n",
    "    'DSC 6.7B',\n",
    "    'DSC 33B',\n",
    "    'CL 7B',\n",
    "    'CL 13B',\n",
    "    'CL 34B',\n",
    "    'GPT 3.5',\n",
    "    'GPT 4'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'java'\n",
    "instances = json.load(open(f'../data/input/CoderEval4{language.capitalize()}.json'))['RECORDS']\n",
    "id_generatedby_touse = list(pd.read_csv(f'../constants/{language}_id_generatedby_touse.csv').id_generatedby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the prompt and the language you want to analyze\n",
    "input_file = '../../data/code_generation/results/cg_judgement_java_automatedCoT.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and add \"level\" field\n",
    "judgments = pd.read_csv(input_file)\n",
    "judgments['id_generatedby'] = judgments['target_id'] + '_' + judgments['generated_by']\n",
    "level = []\n",
    "for tid in judgments.target_id:\n",
    "    l = [i['level'] for i in instances if i['_id'] == str(tid)][0]\n",
    "    level.append(l)\n",
    "judgments['level'] = level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### RUN THIS CELL IF YOU WANT TO CONSIDER ONLY METHODS WITH NO EXTERNAL DEPENDENCIES ###########\n",
    "judgments = judgments.loc[(judgments.level == 'self_contained') | (judgments.level == 'slib_runnable')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape before cleaning : {judgments.shape}.')\n",
    "judgments.dropna(subset = 'generated_code', inplace = True) # exlude all the cases in which the model in charge of the code generation was not able to produce a valid prediction\n",
    "judgments = judgments.loc[judgments.id_generatedby.isin(id_generatedby_touse)]\n",
    "print(f'Shape after cleaning : {judgments.shape}.')\n",
    "print()\n",
    "\n",
    "for col in [c for c in judgments.columns if '_rating' in c]:\n",
    "    judgments[col] = judgments[col].apply(lambda x : int(x) if x != '-' else x)\n",
    "    print('{0} was not able to generate a valid judgement {1} times out of {2}'.format(col.split('_rating')[0], judgments.loc[judgments[col] == '-'].shape[0], judgments.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_cols = 4, 2\n",
    "fig, axs = plt.subplots(num_rows, num_cols, sharex = True, sharey = True, figsize = (8,15))\n",
    "models = ['deepseek-coder-1.3b-instruct', 'deepseek-coder-6.7b-instruct', 'deepseek-coder-33b-instruct', 'CodeLlama-7b-Instruct-hf', 'CodeLlama-13b-Instruct-hf', 'CodeLlama-34b-Instruct-hf', 'gpt-3.5-turbo', 'gpt-4-turbo']\n",
    "model_label = MODEL_NAMES\n",
    "\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        model = models[num_cols * i + j]\n",
    "        judgments_temp = judgments.loc[judgments[f'{model}_rating'] != '-']\n",
    "        judgments_temp[f'{model}_rating'] = judgments_temp[f'{model}_rating'].apply(lambda x : int(x))\n",
    "        actual = judgments_temp.is_pass\n",
    "        predicted = judgments_temp[f'{model}_rating']\n",
    "        confusion_matrix = metrics.confusion_matrix(actual, predicted, normalize = 'true')\n",
    "        cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "        cm_display.plot(cmap = 'Greys', ax = axs[i,j], values_format = '.2f')\n",
    "        cm_display.im_.set_clim(0, 1)\n",
    "        axs[i,j].set_title(f'{model_label[num_cols * i + j]} ({judgments_temp.shape[0]})')\n",
    "        axs[i,j].set_xlabel('') if i != num_rows - 1 else axs[i,j].set_xlabel('Judged', fontsize = 12)\n",
    "        axs[i,j].set_ylabel('') if j != 0 else axs[i,j].set_ylabel('Test output', fontsize = 12)\n",
    "\n",
    "        axs[i,j].set_yticks(ticks = [0, 1], labels = ['fail', 'pass'])\n",
    "        axs[i,j].set_xticks(ticks = [0, 1], labels = ['wrong', 'correct'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa score\n",
    "Evaluate the Kappa agreement score between the {0, 1} series of the is_pass field and the series of ratings given by the LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_kappa = []\n",
    "for col in [c for c in judgments.columns if '_rating' in c]:\n",
    "    judgments_kappa = judgments.loc[judgments[col] != '-']\n",
    "    score = cohen_kappa_score(judgments_kappa['is_pass'], judgments_kappa[col].astype(int))\n",
    "    bool_kappa.append('{0:.2f}'.format(score, judgments_kappa.shape[0]))\n",
    "    print(col, ':', score, judgments_kappa.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write csv for statistical analysis\n",
    "# P-VALUE LLM \n",
    "# - self judgement VS judgement of all other LLMs\n",
    "# - self judgement VS judgement of all other LLMs not belonging to the same family\n",
    "# - self judgement VS judgement of human witten functions\n",
    "\n",
    "for col in [c for c in judgments.columns if '_rating' in c]:\n",
    "    judgments_temp = judgments.loc[judgments[col] != '-'][['generated_by', col, 'is_pass']]\n",
    "    judge = col.split('_rating')[0]\n",
    "    family_name = col.split('-')[0]\n",
    "\n",
    "    temp = judgments_temp.loc[judgments_temp.generated_by == judge]\n",
    "    itsown = np.array(temp[col] - temp.is_pass) # judjements that the model in judge as given to the candidates proposed by judge itself\n",
    "    \n",
    "    temp = judgments_temp.loc[(~judgments_temp.generated_by.str.contains(family_name)) & (judgments_temp.generated_by != 'human_written')]\n",
    "    all_but_family = np.array(temp[col] - temp.is_pass) # judjements that the model in judge as given to the candidates proposed by all the other LLMs not belonging to its family\n",
    "    \n",
    "    temp = judgments_temp.loc[(judgments_temp.generated_by != judge) & (judgments_temp.generated_by != 'human_written')]\n",
    "    all_others = np.array(temp[col] - temp.is_pass) # judjements that the model in judge as given to the candidates proposed by all the other LLMs\n",
    "    \n",
    "    temp = judgments_temp.loc[judgments_temp.generated_by == 'human_written']\n",
    "    human = np.array(temp[col] - temp.is_pass) # judjements that the model in judge as given to the target methods\n",
    "    while itsown.shape[0] < all_others.shape[0]:\n",
    "        itsown = np.concatenate((itsown, np.array([np.nan])))\n",
    "    while all_but_family.shape[0] < all_others.shape[0]:\n",
    "        all_but_family = np.concatenate((all_but_family, np.array([np.nan])))\n",
    "    while human.shape[0] < all_others.shape[0]:\n",
    "        human = np.concatenate((human, np.array([np.nan])))\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'Model' : itsown,\n",
    "        'all_LLM' : all_others,\n",
    "        'all_Minus' : all_but_family,\n",
    "        'humans' : human\n",
    "    }).to_csv(f'../../2_llms_as_judge/results/tse/cg_judgement/Ranalysis/{judge}_judgments-vs-others.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['deepseek-coder-1.3b-instruct', 'deepseek-coder-6.7b-instruct', 'deepseek-coder-33b-instruct', 'CodeLlama-7b-Instruct-hf', 'CodeLlama-13b-Instruct-hf', 'CodeLlama-34b-Instruct-hf', 'gpt-3.5-turbo', 'gpt-4-turbo', 'human_written']\n",
    "rating_cols = [c for c in judgments.columns if '_rating' in c]\n",
    "battle_ship = np.zeros((len(rating_cols), len(models)))\n",
    "\n",
    "for row, judge in enumerate(rating_cols):\n",
    "    for col, candidate in enumerate(models):\n",
    "        bool_temp = judgments.loc[(judgments.generated_by == candidate) & (judgments[judge] != '-')]\n",
    "        battle_ship[row, col] = (bool_temp[judge] - bool_temp.is_pass).sum() / bool_temp.shape[0]\n",
    "\n",
    "d = {k : v for k, v in zip(models, battle_ship)}\n",
    "df = pd.DataFrame(d).T\n",
    "df.columns = models\n",
    "\n",
    "df.columns = MODEL_NAMES_SHORT + ['Human Written']\n",
    "df.index = MODEL_NAMES_SHORT\n",
    "print(df.to_latex(index = True, float_format = \"{:.2f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
