# LLM-as-a-judge-for-SE
This repository is the replication package of the work **"On the Effectiveness of LLM-as-a-judge for Software-related Tasks"**. The purpose of this repository is to provide the data and discuss the pipeline that we used to run this study.

## LLM as a judge on Code Generation task

This part of our work is dedicated to aswering the following research question: _Given a textual description of a java method and its signature, to what extent are LLMs good at judging the correctess of a provided candidate java method?_

### Pipeline

For this part of our work we rely on the **CoderEval ICSE'24** dataset for **Java**, which can be found [here](https://github.com/CoderEval/CoderEval).

**1) Dataset cleaning:**

Clean the CoderEval dataset from instances with noisy or wrong test cases:
- remove instances with target implementation not passing the test;
- remove the instances which pass the test with an empty body (only one comment is present);
- remove instances which pass the test with a trivial body: only the return statement is present. For example ```return 0;``` if the return type is ```int```.;
  
Let ```N = 184``` be the number of valid instances present in the CoderEval dataset.

**2) Code generation (```scripts/code_generation.py``` and ```scripts/code_generation_ChatGPT.py```):**
- for each of the ```N``` valid instances, we get the predictions from 8 LLMs (```deepseek-ai/deepseek-coder-1.3b-instruct```, ```deepseek-ai/deepseek-coder-6.7b-instruct```, ```deepseek-ai/deepseek-coder-33b-instruct```, ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```);
- the prompts for the code generation task are reported in ```prompts/prompt-code-generation.tex``` and ```prompts/prompt-code-generation_ChatGPT.tex``` (for the ChatGPT models only).
- the extraction of the method from the raw output of the model is also performed in these scripts.
- to query the models belonging to the DeepSeek Coder and CodeLlama families we relied on the [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated) provided by Hugging Face; whereas to query the GPT models we took advantage of the [API](https://platform.openai.com/docs/overview) provided by OpenAI.

Since some LLMs may fail at the code generation task (i.e., from their output no valid methods can be extracted), the total amount of candidates is at most ```N x number_LLMs = 184 x 8 = 1,472```. Let ```M = 1,431``` be the total number of valid candidates generated by LLMs.

**3) Outcome of CoderEval tests:**

The candidates go through the CoderEval test cases and for each of them we associate ```1``` if the candidate passes the tests and ```0``` otherwise. Information about the outcome of the CoderEval tests can be found in the ```data/results/cg_judgement_java_boolean_human.csv``` file (```is_pass``` field). To run the tests we relied on the instructions present in the [CoderEval repository](https://github.com/CoderEval/CoderEval).
  	
**4) Each LLM acts as a judge on the code generation task (```scripts/judge_code_generation.py```):**

Each model as a judge is asked to assert the quality of a given candidate based on the description and the signature of a target method. The judge is asked to provide a binary answer (i.e., ```1``` if it thinks that the candidate is correct and ```0``` otherwise). Note that each model as a judge provides a judgment both for the ```M = 1,431``` java methods automatically generated and for the ```N = 184``` target methods written by humans (which are valid implementations by construction). Therefore, each LLM-as-a-judge was asked to judge ```M + N = 1,615``` different candidates in total. The judge is asked both for a ```# Rating``` and a ```# Rationale``` (i.e., an explanation of the ```# Rating```). The optimized prompt that we fed to all the LLMs-as-a-judge is ```prompts/prompt-judge-code-generation-boolean.tex```.

**5) Extraction of the ```# Rating``` and the ```# Rationale``` for each judgement (```scripts/extract_rating_rationale_bool.py```):**

This script is used to extract the ```# Rating``` and ```# Rationale``` information from the raw output of the models as judges. For each model some manually crafted heuristics are applied to the ```1,615``` model outputs.

All outputs are manually analyzed to extract information that were missed by the heuristics. Note that, in the cases in which the judge fails to provide a valid judgement (i.e., either the ```# Rating``` or the ```# Rationale``` are not present in the model output), both the ```# Rating``` and the ```# Rationale``` are set to "-", and the judgment is considered invalid.

Results are in ```data/results/cg_judgement_java_boolean_human.csv```.

**6) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cg.ipynb```. To assess the quality of the judgments of each LLM we use Cohen's Kappa agreement, and to detect any trace of self-bias in the judgments of the models we evaluate the average of the differences of the predictions of the models (```1``` if the implementation is considered of high quality and ```0``` otherwise) and the pass/fail (```1``` or ```0```) ground truth.

**7) Qualitative manual analysis of judgement failure cases:**

The goal is to extract a sample of cases in which each LLM-as-a-judge fails at judging a candidate method. For each judge we sample ```15``` false positives (i.e., cases in which the candidate is evaluated positively by the judge, but does not pass the relative test of the CoderEval dataset) and ```15``` false negatives (i.e., candidates that are evaluated negatively by the judge, but do pass the relative test of the CoderEval dataset). We sample ```15``` examples (when present) per judge and per category of failure (false positives/negatives). We define a set of categories for which the judge may fail at judging the candidate method and then we assign each case of failure to one or more of our categories. The output of our manual analysis is reported in ```cg_MA.csv```, whereas the number of occurrences of each category is reported in ```cg_MA_false_positives.csv``` and ```cg_MA_false_negatives.csv```. These files are in the ```data/code_generation_manual_analysis``` folder.

## Creation of the Code Summarization benchmark for Java

For this part of our work we create a code summarization benchmark for Java starting from the CoderEval dataset discussed above. Our dataset features human judgements of ```594``` summaries. To build the dataset, we selected from the CoderEval benchmark the top-100 Java methods in terms of number of statements they feature. We decided to focus on the longest methods since those are the ones for which a good summary is likely to make a difference in terms of code comprehensibility and, thus, assessing the quality of summaries for these methods may make more sense. Among these ```100``` methods we found one that was a duplicate and was thus removed from the set, leaving us with ```99``` methods. For each of them, we have the associated code summary written by the original developer of the method, already featured in the CoderEval dataset. Furthermore, we asked five LLMs (i.e., CodeLlama 7B, 13B, and 34B, GPT-3.5-turbo and GPT-4-turbo) to generate a summary for each of these ```99``` methods, leading to the total of ```99``` (manually written) + ```99Ã—5``` automatically generated) = ```594``` summaries. The prompt used to generate code summaries with the LLMs is documented in ```prompts/prompt-summary-generation.tex```. We asked 3 different experienced Java developers to evaluate the quality of the summaries in terms of ```content adequacy``` (the extent to which the comment summarizes all information that can be inferred from the source code), ```conciseness``` (the extent to which the comment contains unnecessary information) and ```fluency & understandability``` (the extent to which the comment is easy to understand). This manual analysis leaves us with ```3``` scores (from 1 to 5) for ```content adequacy```, ```3``` for ```conciseness``` and ```3``` for ```fluency & understandability``` for every of the ```594``` summaries. We asked the human raters to evaluate each aspect independently from one another (i.e., a summary can be considered concise even if the information it contains is wrong). Our dataset is available at ```data/cs_benchmark/CS-dataset.csv```.

## LLM-as-a-judge on Code Summarization task
### Pipeline

We use the dataset described before to run our judgements on code summarization.

**1) LLM-as-a-judge for code summarization (```scripts/judge_code_summarization.py```):**

- for each instance of the dataset (i.e., each method-summary pair), a chosen LLM-as-a-judge is asked to assert the quality of the summary with respect to the method. The judge is asked to give a ```# Rating``` and a ```# Rationale``` for 3 different aspects: ```content adequacy```, ```conciseness``` and ```fluency & understandability```. The optimized prompt that we fed to the LLMs-as-a-judge is reported in ```prompts/prompt-judge-code-summarization.tex```;
- for this task we select only ```5``` LLMs which will play the role of the judge (namely ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```) because unfortunately the LLMs belonging to the DeepSeek Coder family often give an invalid output;
- Hugging Face Inference Endpoints and OpenAI API are exploited to query the LLMs, as explained above.

**2) ```# Rating``` extraction for all quality aspects (```scripts/extract_rating_ccf.py```):**

- a set of heuristics is used to extract the values for ```# Rating``` for all the quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```) from the raw output of the LLMs;
- manual extraction is performed when the heuristics fail;

**3) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cs.ipynb```. To assess the quality of the judgments of each LLM we use [Krippendorff's alpha](https://www.k-alpha.org), and to detect any trace of self-bias in the judgments of the models we evaluate the average of the differences of the predictions of the models and the ground truth. Both the predictions and the "true" label range from 1 (very low quality) to 5 (very high quality). As ground truth the median of the ratings given by the 3 human developers is used.

## Data (```data/```):

**1) ```results/cg_judgement_java_boolean_human.csv```:**

This is the overall output of the code generation judgement task. It is a spreadsheet containing the following fields:

- ```target_id``` : alphanumeric string associated to each valid instance of CoderEval;
- ```generated_by``` : the LLM which generated the candidate code (or ```human_written``` if the method was the target);
- ```generated_code``` : the candidate method (or target implementation if ```human_written```);
- ```is_pass``` : ```1``` if the candidate passes the test of the CoderEval dataset, ```0``` otherwise (target methods are associated with ```1``` by construction);
- ```{LLM1}_rating``` : the rating given by the model-as-a-judge;
- ```{LLM1}_rationale``` : the rationale given by the model-as-a-judge;
- ...

The last two columns are repeated for each LLM-as-a-judge.

**2) ```code_generation_manual_analysis/cg_MA.csv```:**

Results of the manual analysis for code generation judgement failures. To assign one or more reasons of failure (i.e., ```category-Final1```, ```category-Final2```, etc...), we take into consideration the docstring (```docstring```) and the candidate implementation (```generated_code```) which were fed to the LLM-as-a-judge (```judge_model```) and the judgment explanation (```rationale```) of the judge. By considering these three elements plus the ```reason_of_failure``` (i.e., ```false_negative``` if the LLM-as-a-judge classified as wrong a candidate implementation which is actually correct, and ```false_positive``` in the opposite scenario), we manually assign a category to the misclassification. Let us discuss the second row of the file ```cg_MA.csv``` for the sake of clarity: ```CodeLlama-34b-Instruct-hf``` is asked to assess the correctess of the candidate implementation (```generated_code```) given a textual description of the method (```Return an EMPTY_BOOLEAN_OBJECT_ARRAY array for a null or empty input array.```) and the its signature (```public static Boolean[] nullToEmpty(final Boolean[] array){```). The LLM-as-a-judge misjudges the candidate implementation assessing it as wrong. The rationale for this choice is ```The candidate implementation does not return an empty array when the input array is null.```. By comparing the docstring, the candidate implementation and the rationale we can conclude that the model's misjudgment is due to ```Misunderstanding of code statements```. In other words, the model is not able to label the correctness of the candidate because it misunderstands the statements of the candidate implementations (i.e., the candiate implementation actually returns an empty array when the input array is null, but the model is not able to detect it).

**3) ```code_generation_manual_analysis/cg_MA_false_positives.csv``` and ```code_generation_manual_analysis/cg_MA_false_negatives.csv```:**

The most frequent reasons (i.e., categories) why LLMs-as-a-judge fail at the code generation judgment task are reported in these files.

**4) ```results/cs/{model_name}_CCF.csv```:**

These files contain the raw output of the models when prompted to do the code summarization judgment task (```model_output``` column) and also their judgments for the three quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```), extracted with the heuristics. 

## Complementary results (```data/complementary_results/```):

**1) ```cg_5level_prompt.pdf```:**

Results for code generation when the model are asked to give a 5-level judgment, i.e., rate the quality of the solution on a scale from 1 to 5 (see ```prompts/prompt-judge-code-generation-5level.tex```).

**2) ```cg_norationale_prompt.pdf```:**

Results for code generation when the model are asked to give a binary judgment, like the one reported in the paper, but they are not asked for a rationale.

**3) ```cg_selfcontained```:**

Results for code generation when considering only the CoderEval methods with no external dependencies.

**4) ```cs_instruction_prompt.pdf```:**

Results for code summarization, but prompting to the LLMs-as-a-judge the exact same guidelines given to the human raters when evaluating the quality of the three aspects for each summary (see ```prompts/prompt-judge-code-summarization-instructions.tex```).

**5) ```cs_meanvsmedian.pdf```:**

Results for code summarization, but using the mean of the human ratings as ground truth instead of the median.

**6) ```cs_ranking_con_flu.pdf```:**

Ranking of the Generators of Summaries according to each Judge for Conciseness and Fluency & Understandability.
