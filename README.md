# LLM-as-a-judge-for-SE
This repository is the replication package of the work **"On the Effectiveness of LLM-as-a-judge for Code Generation and Summarization"**. The purpose of this repository is to provide the data and discuss the pipeline that we used to run this study.

## LLM-as-a-judge on Code Generation task

This part of our work is dedicated to answering the following research question: _Given a textual description of a java method and its signature, to what extent are LLMs good at judging the correctess of a provided candidate method?_

### Pipeline

For this part of our work we rely on the **CoderEval ICSE'24** dataset for **Java** and **Python**, which can be found [here](https://github.com/CoderEval/CoderEval). Below, we present the pipeline considering as a concrete example the steps taken when running the experiments for **Java**. Note that the pipeline for **Python** is 100% analogous.

**1) Dataset cleaning:**

Clean the CoderEval dataset from instances with noisy or wrong test cases:
- remove instances with target implementation not passing the test;
- remove the instances which pass the test with an empty body (only one comment is present);
- remove instances which pass the test with a trivial body: only the return statement is present. For example ```return 0;``` if the return type is ```int```. (for Python we simply ```return None```);
  
Let ```N = 184``` be the number of valid instances present in the CoderEval Java dataset.

**2) Code generation (```code/scripts/tse-code_generation.py```):**
- for each of the ```N``` valid instances, we get the predictions from 8 LLMs (```deepseek-ai/deepseek-coder-1.3b-instruct```, ```deepseek-ai/deepseek-coder-6.7b-instruct```, ```deepseek-ai/deepseek-coder-33b-instruct```, ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```);
- the prompts for the code generation task are reported in ```prompts/code_generation/prompt-code-generation.tex``` and ```prompt-code-generation_ChatGPT.tex``` (for the ChatGPT models only).
- the extraction of the method from the raw output of the model is also performed in these scripts.
- to query the models belonging to the DeepSeek Coder and CodeLlama families we relied on the [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated) provided by Hugging Face; whereas to query the GPT models we took advantage of the [API](https://platform.openai.com/docs/overview) provided by OpenAI.

Since some LLMs may fail at the code generation task (i.e., from their output no valid methods can be extracted), the total amount of candidates is at most ```N x number_LLMs = 184 x 8 = 1,472```. Also consider that some implementations may be syntactically incomplete or trivially incorrect (e.g., only one comment like ```// your implementation goes here```). These implementations are discarded as well, along with duplicate implementations. Let ```M = 1,221``` be the total number of valid candidates generated by LLMs for Java.

**3) Outcome of CoderEval tests:**

The candidates go through the CoderEval test cases and for each of them we associate ```1``` if the candidate passes the tests and ```0``` otherwise. Information about the outcome of the CoderEval tests can be found in the ```data/code_generation/results``` files (```is_pass``` field). To run the tests we relied on the instructions present in the [CoderEval repository](https://github.com/CoderEval/CoderEval).
  	
**4) Each LLM acts as a judge on the code generation task (```code/scripts/tse-judge_code_generation.py```):**

Each model-as-a-judge is asked to assert the quality of a given candidate based on the description and the signature of a target method. The judge is asked to provide a binary answer (i.e., ```1``` if it thinks that the candidate is correct and ```0``` otherwise). Note that each model-as-a-judge provides a judgment both for the ```M = 1,221``` java methods automatically generated and for the ```N = 184``` target methods written by humans (which are valid implementations by construction). Therefore, each LLM-as-a-judge was asked to judge ```M + N = 1,405``` different candidates in total. We try with 4 different prompts, namely ```zero-shot```, ```zero-shot W/O rationale```, ```automated Chain-of-Thought```, ```slow-thinking``` (inspired by [Tong et al.](https://arxiv.org/abs/2410.02184)). The optimized prompts that are available in ```prompts/code_generation```.

**5) Postprocess the raw judgments of the models:**

A script is used to postprocess the row output of the models as judges, in order to extract the ```# Rating``` (and the ```# Rating``` for the ```zero-shot``` prompt). For each model some manually crafted heuristics are applied to the ```1,405``` model outputs.

All outputs are manually analyzed to extract information that were missed by the heuristics. Note that, in the cases in which the judge fails to provide a valid judgement (i.e., either the ```# Rating``` or the ```# Rationale``` are not present in the model output) the judgment is considered invalid.

Results are in ```data/code_generation/results```.

**6) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cg.ipynb```. To assess the quality of the judgments of each LLM we use Cohen's Kappa agreement, and to detect any trace of self-bias in the judgments of the models we evaluate the average of the differences of the predictions of the models (```1``` if the implementation is considered of high quality and ```0``` otherwise) and the pass/fail (```1``` or ```0```) ground truth.

**7) Qualitative manual analysis of judgement failure cases:**

The goal is to extract a sample of cases in which each LLM-as-a-judge fails at judging a candidate method. For each judge we sample ```15``` false positives (i.e., cases in which the candidate is evaluated positively by the judge, but does not pass the relative test of the CoderEval dataset) and ```15``` false negatives (i.e., candidates that are evaluated negatively by the judge, but do pass the relative test of the CoderEval dataset). We use the **```zero-shot``` prompt** because it is the only one which explicitly asks the model-as-a-judge to provide a rationale to justify its rating. We sample ```15``` examples (when present) per judge and per category of failure (false positives/negatives). We define a set of categories for which the judge may fail at judging the candidate method and then we assign each case of failure to one or more of our categories. The number of occurrences of each category is reported in ```additional_results/code_generation/cg-failing-reasons-by-LLM.pdf``` for both Java and Python.

## Creation of the Code Summarization benchmark for Java and Python

Let us discuss the steps taken to create the benchmark for Java.

For this part of our work we create a code summarization benchmark for Java starting from the CoderEval dataset discussed above. Our dataset features human judgements of ```594``` summaries. To build the dataset, we selected from the CoderEval benchmark the top-100 Java methods in terms of number of statements they feature (```scripts/summary_generation.py```). We decided to focus on the longest methods since those are the ones for which a good summary is likely to make a difference in terms of code comprehensibility and, thus, assessing the quality of summaries for these methods may make more sense. Among these ```100``` methods we found one that was a duplicate and was thus removed from the set, leaving us with ```99``` methods. For each of them, we have the associated code summary written by the original developer of the method, already featured in the CoderEval dataset. Furthermore, we asked five LLMs (i.e., CodeLlama 7B, 13B, and 34B, GPT-3.5-turbo and GPT-4-turbo) to generate a summary for each of these ```99``` methods (```scripts/summary_generation.py```), leading to the total of ```99``` (manually written) + ```99Ã—5``` automatically generated) = ```594``` summaries. The prompt used to generate code summaries with the LLMs is documented in ```prompts/prompt-summary-generation.tex```. Note that the raw output of the models was postprocessed with simple regular expressions (mainly, ```r"/\*\*.*?\*/"```).

Afterwards, we asked 3 different experienced Java developers to evaluate the quality of the summaries in terms of ```content adequacy``` (the extent to which the comment summarizes all information that can be inferred from the source code), ```conciseness``` (the extent to which the comment contains unnecessary information) and ```fluency & understandability``` (the extent to which the comment is easy to understand). This manual analysis leaves us with ```3``` scores (from 1 to 5) for ```content adequacy```, ```3``` for ```conciseness``` and ```3``` for ```fluency & understandability``` for every of the ```594``` summaries (```569``` for Python). We asked the human raters to evaluate each aspect independently from one another (i.e., a summary can be considered concise even if the information it contains is wrong). Our benchmarks are available at ```data/code_summarization_benchmark```.

## LLM-as-a-judge on Code Summarization task

This part of our work is dedicated to answering the following research question: _Given a method and a textual summary written with the intent of documenting the method, to what extent are LLMs good at assessing the quality of the summary?_

### Pipeline

We use the datasets described before to run our judgements on code summarization.

**1) LLM-as-a-judge for code summarization (```code/scripts/tse-judge_code_summarization.py```):**

- for each instance of the dataset (i.e., each method-summary pair), a chosen LLM-as-a-judge is asked to assert the quality of the summary with respect to the method.
- We experiment with 4 different prompts, namely ```zero-shot```, ```zero-shot + instructions```, ```automated Chain-of-Thought```, ```automated Chain-of-Thought + instructions```. In all the prompts, the judge is asked to give a ```# Rating``` for 3 different aspects: ```content adequacy```, ```conciseness``` and ```fluency & understandability```. The optimized prompts that we fed to the LLMs-as-a-judge are reported in ```prompts/code_summarization```;
- for this task we select only ```5``` LLMs which will play the role of the judge (namely ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```) because the LLMs belonging to the DeepSeek Coder family often give an invalid output;
- Hugging Face Inference Endpoints and OpenAI API are exploited to query the LLMs, as explained above.

**2) Postprocess the raw judgments of the models:**

- a set of heuristics is used to extract the values for ```# Rating``` for all the quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```) from the raw output of the LLMs;
- manual extraction is performed when the heuristics fail;

**3) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cs.ipynb```. To assess the quality of the judgments of each LLM we use [Krippendorff's alpha](https://www.k-alpha.org), and to detect any trace of self-bias in the judgments of the models we evaluate the average of the differences of the predictions of the models and the ground truth. Both the predictions and the "true" label range from 1 (very low quality) to 5 (very high quality). As ground truth the median of the ratings given by the 3 human developers is used.

## Data (```data/```):

### Code generation (```data/code_generation```):

**1) ```input/CoderEval4Java.json```:**

This is the CoderEval benchmark for Java, as it was published by the authors. [Here](https://github.com/CoderEval/CoderEval) also the benchmark for Python can be found.

**2) ```predictions/CodeLlama-7b-Instruct-hf.jsonl```:**

Here we report an example of the code completions generated by one of our models. These completions are used as candidates, whose correctness must be evaluated by the LLMs-as-a-judge.

**3) ```results/cg_judgement_java_zeroshot.csv```:**

This is the overall output of the code generation judgement task for a given prompt. It is a spreadsheet containing the following fields:

- ```target_id``` : alphanumeric string associated to each valid instance of CoderEval;
- ```generated_by``` : the LLM which generated the candidate code (or ```human_written``` if the method was the target);
- ```generated_code``` : the candidate method (or target implementation if ```human_written```);
- ```is_pass``` : ```1``` if the candidate passes the test of the CoderEval dataset, ```0``` otherwise (target methods are associated with ```1``` by construction);
- ```{LLM1}_rating``` : the rating given by the model-as-a-judge;
- ```{LLM1}_rationale``` : the rationale given by the model-as-a-judge;
- ...

The last two columns are repeated for each LLM-as-a-judge.

### Code summarization (```data/code_summarization/results```):

**1) ```java/zeroshot/{model_name}_CCF.csv```:**

These files contain the raw output (```model_output``` column) of the models when prompted to do the code summarization judgment task and also their judgments for the three quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```), extracted with the heuristics.

**2) ```input/csdata4visualization.csv```:**

This file merges the information concerning the human rating (i.e., ```CA_1```, ```CA_2```, ```CA_3```) and the prediction of the 5 LLMs for the given method-summary pair.

## Additional results (```additional_results/code_generation```):

**1) ```cg-results-achieved-with-all-prompts.pdf```:**

Same results reported in the paper, achieved with the three prompts that we did not present in the paper (i.e., ```zero-shot```, ```zero-shot W/O rationale``` and ```slow-thinking```).

**2) ```cg-failing-reasons-by-LLM.pdf```:**

The number of occurrences of each category of failure that we defined (see point 7 of the code generation pipeline). Note that more than one label can be applied to each case of failure. The categories are the following: 
- ```Ambiguous docstring```: The docstring does not provide enough information to the model for the assessment.
- ```Test result unreliable```: Only false_negative: The code passess the test, but it's because the test is not good. The model is actually right here.
- ```Limited coding context```: For false_negative: The LLM complains about code elements used in the function but not visible to it e.g., variables used but not defined in the function, but at class-level. For false_positive: The LLM is not aware that certain code elements must be used to obtain a correct implementation (e.g., invoking an already existing function, using an already existing object) and, thus, misjudges the function as working even though this is not the case.
- ```Uncought wrong behavioral```: Only false_positive: The behavior is wrong due to some statements, but the LLM does not detect that.
- ```Misunderstanding of code statements```: Only false_negative: The LLM assesses the code as wrong due to misunderstanding of the implemented function, which is actually correct.
- ```Artificial hallucination```: The rationale provided by the LLM mentions code elements/implementation choices which are completely unrelated to the assessed function (i.e., they do not appear) or, instead, says that certain requirements have not been implemented but instead they are there.
- ```Shallow description```: The rationale provided by the LLM provides a shallow description of the generated code and it doesnt contain sufficient details to judge the correctness. 
- ```Focus on non-functional requirements```: The LLM rational poses the focus on non-functional requirements (e.g., performance) rather than on the code correctness, resuting in a misjudgment.
- ```Misintepreted implementation requirements```: The LLM misjudges the function because it does not undertand the input requirement (e.g., it says "the method should return X if the parameter is null", but the input description says that it should return Y in that case).

**3) ```cg-judgment-of-self-contained-functions-only.pdf```:**

Results for code generation (automated Chain-of-Thought prompt) when considering only the CoderEval methods with no external dependencies (these are the instances of the CoderEval dataset with either ```"level": "self_contained"``` or ```"level": "slib_runnable"```).

**4) ```cg-self-bias-automatic-CoT-python.pdf```:**

Self-bias table for Python obtained with automated Chain-of-Thought prompt.

## Additional results (```additional_results/code_summarization```):

**1) ```cg-results-achieved-with-all-prompts.pdf```:**

Same results reported in the paper, achieved with the three prompts that we did not present in the paper (i.e., ```zero-shot + instructions```, ```automated Chain-of-Thought```, ```automated Chain-of-Thought + instructions```).

**2) ```cs-ranking-of-generators-by-judge.pdf```:**

Ranking of the generators of summaries according to each LLM-as-a-judge for all the prompts.

**3) ```cs-scatter-plot-zero-shot-python.pdf```:**

Code summarization results for Python obtained with zero-shot prompt.

**4) ```cs-self-bias-zero-shot-python.pdf```:**

Self-bias table for Python obtained with zero-shot prompt.
