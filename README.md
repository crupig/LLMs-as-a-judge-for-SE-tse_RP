# LLM-as-a-judge-for-SE
This repository is the replication package of the work **"On the Effectiveness of LLM-as-a-judge for Software-related Tasks"**. The purpose of this repository is to provide the data and discuss the pipeline that we used to run this study.

## LLM as a judge on Code Generation task
### Pipeline

For this part of our work we rely on the **CoderEval ICSE'24** dataset for **Java**, which can be found [here](https://github.com/CoderEval/CoderEval).

**1) Dataset cleaning:**

Clean the CoderEval dataset from instances with noisy or wrong test cases:
- remove instances with target implementation not passing the test;
- remove the instances which pass the test with an empty body (only one comment is present);
- remove instances which pass the test with a trivial body: only the return statement is present. For example ```return 0;``` if the return type is ```int```.;
  
Let ```N = 184``` be the number of valid instances present in the CoderEval dataset.

**2) Code generation (```scripts/code_generation.py``` and ```scripts/code_generation_ChatGPT.py```):**
- for each of the ```N``` valid instances, we get the predictions from 8 LLMs (```deepseek-ai/deepseek-coder-1.3b-instruct```, ```deepseek-ai/deepseek-coder-6.7b-instruct```, ```deepseek-ai/deepseek-coder-33b-instruct```, ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```);
- the prompts for the code generation task are reported in ```prompts/prompt-code-generation.tex``` and ```prompts/prompt-code-generation_ChatGPT.tex``` (for the ChatGPT models only).
- the extraction of the method from the raw output of the model is also performed in these scripts.

Since some LLMs may fail at the code generation task (i.e., from their output no valid methods can be extracted), the total amount of candidates is at most ```N x number_LLMs = 184 x 8 = 1,472```. Let ```M = 1,431``` be the total number of valid candidates generated by LLMs.

**3) Outcome of CoderEval tests:**

The candidates go through the CoderEval test cases and for each of them we associate 1 if the candidate passes the tests and 0 otherwise. Information about the outcome of the CoderEval tests can be found in the ```data/results/cg_judgement_java_boolean_human.csv``` file. To run the tests we relied on the instructions present in the CoderEval repository.
  	
**4) Each LLM acts as a judge on the code generation task (```scripts/judge_code_generation.py```):**

Each model as a judge is asked to assert the quality of a given candidate based on the description and the signature of a target method. The judge is asked to provide a binary answer (i.e., 1 if it thinks that the candidate is correct and 0 otherwise). Note that each model as a judge provides a judgment both for the ```M = 1,431``` java methods automatically generated and for the ```N = 184``` target methods written by humans (which are valid implementations by construction). Therefore, each LLM-as-a-judge was asked to judge ```M + N = 1,615``` different candidates in total. The judge is asked both for a ```# Rating``` and a ```# Rationale``` (i.e., an explanation of the ```# Rating```). The optimized prompt that we fed to the LLMs as a judge is ```prompts/prompt-judge-code-generation-boolean.tex```.

**5) Extract the ```# Rating``` and the ```# Rationale``` for each judgement (```scripts/extract_rating_rationale_bool.py```):**

This script is used to extract the ```# Rating``` and ```# Rationale``` information from the raw output of the models as judges. For each model some manually crafted heuristics are applied to the ```1,615``` model outputs.

**6) Rating and Rationale manual check:**

All outputs from point ```5)``` are manually analyzed to extract information that were missed by the heuristics. Note that, in the cases in which the judge fails to provide a valid judgement (i.e., either the ```# Rating``` or the ```# Rationale``` are not present in the model output), both the ```# Rating``` and the ```# Rationale``` are set to "-".

Results are in ```data/results/cg_judgement_java_boolean_human.csv```.

**7) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cg.ipynb```. To assess the quality of the judgments of each LLM we use Cohen's Kappa agreement, and to detect any trace of self-bias in the judgments of the models we evaluate the average of the differences of the predictions of the models (1 if the implementation is considered of high quality and 0 otherwise) and the pass/fail (1/0) ground truth.

**8) Qualitative manual analysis of judgement failure cases:**

The goal is to extract a sample of cases in which each LLM-as-a-judge fails at judging a candidate method. For each judge we sample 15 false positives (i.e., cases in which the candidate is evaluated positively by the judge, but does not pass the relative test of the CoderEval dataset) and false negatives (i.e., candidates that are evaluated negatively by the judge, but do pass the relative test of the CoderEval dataset). We sample 15 examples (when present) per judge and per category of failure (false positives/negatives). We define a set of categories for which the judge may fail at judging the candidate method and then we assign each case of failure to one or more of our categories. The output of our manual analysis is reported in ```cg_MA.csv```, whereas the number of occurrences of each category is reported in ```cg_MA_false_positives.csv``` and ```cg_MA_false_negatives.csv```. These files are in the ```data/code_generation_manual_analysis``` folder.

## Creation of the Code Summarization benchmark for Java

For this part of our work we create a code summarization benchmark for Java starting from the CoderEval dataset discussed above. Our dataset features human judgements of 594 summaries. To build the dataset, we selected from the CoderEval benchmark the top-100 Java methods in terms of number of statements they feature. We decided to focus on the longest methods since those are the ones for which a good summary is likely to make a difference in terms of code comprehensibility and, thus, assessing the quality of summaries for these methods may make more sense. Among these 100 methods we found one that was a duplicate and was thus removed from the set, leaving us with 99 methods. For each of them, we have the associated code summary written by the original developer of the method. Also, we asked five LLMs (i.e., CodeLlama 7B, 13B, and 34B, GPT-3.5-turbo and GPT-4-turbo) to generate a summary for each of these 99 methods, leading to the total of 99 (manually written) + 99Ã—5 automatically generated) = 594 summaries. The prompt used to generate code summaries with the LLMs is documented in ```prompts/prompt-summary-generation.tex```. We asked 3 different experienced Java developers to evaluate the quality of the summaries in terms of ```content adequacy``` (the extent to which the comment summarizes all information that can be inferred from the source code), ```conciseness``` (the extent to which the comment contains unnecessary information) and ```fluency & understandability``` (the extent to which the comment is easy to understand). This manual analysis leaves us with 3 scores (from 1 to 5) for ```content adequacy```, 3 for ```conciseness``` and 3 for ```fluency & understandability``` for every of the 594 summaries. We asked the human raters to evaluate each aspect independently from one another (i.e., a summary can be considered concise even if the information it contains is wrong). Our dataset is available at ```data/cs_benchmark/CS-dataset.csv```.

## LLM-as-a-judge on Code Summarization task
### Pipeline

We use the dataset described before to run our judgements on code summarization.

**1) LLM-as-a-judge for code summarization (```scripts/judge_code_summarization.py```):**

- for each snippet-summary pair, a chosen model as a judge is asked to assert the quality of the summary with respect to the snippet. The judge is asked to give a ```# Rating``` and a ```# Rationale``` for 3 different aspects: ```content adequacy```, ```conciseness``` and ```fluency & understandability```. The optimized prompt that we fed to the LLMs-as-a-judge is reported in ```prompts/prompt-judge-code-summarization.tex```;
- for this task we select only 5 LLMs which will play the role of the judge (namely ```codellama/CodeLlama-7b-Instruct-hf```, ```codellama/CodeLlama-13b-Instruct-hf```, ```codellama/CodeLlama-34b-Instruct-hf```, ```gpt-3.5-turbo```, ```gpt-4-turbo```) because unfortunately the LLMs belonging to the DeepSeek Coder family often give an invalid output.

**2) ```# Rating``` extraction for all quality aspects (```scripts/extract_rating_ccf.py```):**

- a set of heuristics is used to extract the values for ```# Rating``` for all the quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```) from the raw output of the LLMs;
- manual extraction is performed when the heuristics fail;

**3) Quantitative analysis:**

Our quantitative analysis can be found in the notebook ```notebooks/visualization_cs.ipynb```.

## Data (```data/```):

**1) ```results/cg_judgement_java_boolean_human.csv```:**

This is the overall output of the code generation judgement task. It is a spreadsheet containing the following fields:

- ```target_id``` : alphanumeric string associated to each valid instance of CoderEval;
- ```generated_by``` : the LLM which generated the candidate code (or ```human_written``` if the method was the target);
- ```generated_code``` : the candidate method (or target method);
- ```is_pass``` : 1 if the candidate passes the test of the CoderEval dataset, 0 otherwise (target methods are associated with 1 by construction);
- ```{LLM1}_rating``` : the rating given by the model as a judge;
- ```{LLM1}_rationale``` : the rationale given by the model as a judge;
- ...

The last two columns are repeated for each LLM-as-a-judge.

**2) ```code_generation_manual_analysis/cg_MA.csv```:**

Results of the manual analysis for code generation judgement failures.

**3) ```code_generation_manual_analysis/cg_MA_false_positives.csv``` and ```code_generation_manual_analysis/cg_MA_false_negatives.csv```:**

The most frequent reasons (i.e., categories) why LLMs-as-a-judge fail at the code generation judgment task are reported in these files.

**4) ```results/cs/{model_name}_CCF.csv```:**

These files contain the raw output of the models when prompted to do the code summarization judgment task (```model_output``` column) and also their judgments for the three quality aspects (```content adequacy```, ```conciseness``` and ```fluency & understandability```), extracted with the heuristics. 

## Complementary results (```data/complementary_results/```):

**1) ```cg_5level_prompt.pdf```:**

Results for code generation when the model are asked to give a 5-level judgment, i.e., rate the quality of the solution on a scale from 1 to 5 (see ```prompts/prompt-judge-code-generation-5level.tex```).

**2) ```cg_norationale_prompt.pdf```:**

Results for code generation when the model are asked to give a binary judgment, like the one reported in the paper, but they are not asked for a rationale.

**3) ```cg_selfcontained```:**

Results for code generation when considering only the CoderEval methods with no external dependencies.

**4) ```cs_instruction_prompt.pdf```:**

Results for code summarization, but prompting to the LLMs-as-a-judge the exact same guidelines given to the human raters when evaluating the quality of the three aspects for each summary (see ```prompts/prompt-judge-code-summarization-instructions.tex```).

**5) ```cs_meanvsmedian.pdf```:**

Results for code summarization, but using the mean of the human ratings as ground truth instead of the median.

**6) ```cs_ranking_con_flu.pdf```:**

Ranking of the Generators of Summaries according to each Judge for Conciseness and Fluency & Understandability.
