import lizard
from pygments.lexers import JavaLexer
import json
from huggingface_hub import InferenceClient
import requests
import numpy as np
from colorama import Fore
from datetime import datetime
import openai
import pandas as pd

CONSTANTS = json.load(open('../constants/constants_json.json'))

IEP_MODELS = json.load(open('../constants/IEPmodels.json'))

IEP_HEADERS = {
	"Accept" : "application/json",
	"Authorization": f"Bearer {CONSTANTS['Hugging Face Inference Endpoints Token']}",
	"Content-Type": "application/json" 
    }

openai.api_key = json.load(open('../constants/constants_json.json'))['OpenAI API Access Key']


def extract_predictions(prediction_path, batch_ids):
    """
    Description of the Function:
        given a path to a jsonl file containing predictions of the CoderEval dataset, 
        it returns a dataframe containing the target_id and the candidate implementation. 
        It only returns the instances specified in the batch_ids list.
        This function handles the case in which to each target was associated only one candidate (beam = 1).

    Parameters:
        prediction_path (str): path to the jsonl file which contains a set of predictions of the CoderEval dataset;
        batch_ids (list) : contains a subset of the target ids of the CoderEval dataset;

    Returns:
        pandas.DataFrame: dataframe with the following columns
            -> target_id (str): id of the instance;
            -> generated_code (str): a candidate function;
   """
    method_id = []
    implementation = []
    with open(prediction_path, 'r') as file:
        for line in file:
            json_obj = json.loads(line)
            idx = json_obj['_id']
            if idx not in batch_ids:
                continue
            method_id.append(idx)
            implementation.append(json_obj['generate_results'][0]) if len(json_obj['generate_results']) > 0 else implementation.append("")

    d = {
        'method_id' : method_id,
        'code' : implementation
    }

    return pd.DataFrame(d)


def extract_ispass(path):
    """
    Description of the Function:
        extracts the output of the tests of CoderEval and returns a dataframe. This function handles the case in which to each
        target was associated only one candidate (beam = 1).

    Parameters:
        path (str): path to the jsonl file given as output by the test suite of CoderEval;

    Returns:
        pandas.DataFrame: dataframe with the following columns
            -> target_id (str): id of the instance;
            -> generated_code (str): the candidate function that went through the test;
            -> is_pass (boolean): whether the candidate passed the test.
   """
    data = []
    with open(path, 'r') as file:
        for line in file:
            json_obj = json.loads(line)
            data.append(json_obj)

    df = pd.DataFrame(data)
    method_id = [df._id.iloc[i] for i in range(df.shape[0])]
    is_pass = [df.generate_results.iloc[i][0]['is_pass'] for i in range(df.shape[0])]
    generated_code = [df.generate_results.iloc[i][0]['generate_code'] for i in range(df.shape[0])]

    d = {
        'target_id' : method_id,
        'generated_code' : generated_code,
        'is_pass' : is_pass
    }

    return pd.DataFrame(d)


def divide_into_batches(data, batch_size):
    for i in range(0, len(data), batch_size):
        yield data[i : i + batch_size]


def query_inference_endpoint(model, prompt, max_new_tokens):
    """
    Description of the Function:
        makes a call to the inference endpoint of a given model.

    Parameters:
        model (str): name of the model to query;
        prompt (str): model input;
        max_new_tokens (int): maximum number of tokens generated by the model (input tokens excluded).

    Returns:
        str: output of the model (input excluded).
   """
    
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens" : max_new_tokens,
            "return_full_text": False,
            "do_sample": False
        }
    }
    endpoint_url = IEP_MODELS[model]
    output = requests.post(url = endpoint_url, headers = IEP_HEADERS, json = payload).json()

    return output[0]['generated_text']


def ask_chatgpt(prompt, gpt_version, language):
    """
    Description of the Function:
        queries ChatGPT API with a given prompt and returns ChatGPT response.

    Parameters:
        prompt (str): ChatGPT input;
        gpt_version (str): official string representing the ChatGPT version;
        language (str): string indicating a programming language;

    Returns:
        str: ChatGPT output.
   """
    count_max = 0
    while count_max < 20:
        try:
            chatgpt_review = openai.ChatCompletion.create(
                model = gpt_version,
                messages = [
                    {'role' : 'system', 'content' : f'You are an expert {language} developer'},
                    {'role' : 'user', 'content' : prompt}
                ],
                temperature = 0.,
                seed = 123
            )
            break
        
        except:
            count_max += 1
            continue
    
    if count_max == 20:
       print('Number of max try reached.')
       return('-')
    else:
        return(chatgpt_review.choices[0].message.content)
    

def search_function(file_path):
    """
    Description of the Function:
        returns the Start line and End line of all the java methods found in a given input file.

    Parameters:
        file_path (str): path to the file to read.

    Returns:
        list of lists [N x 3]: for each java method found, it returns a list containing the Name of the method, the Start line and the End line of the method.
   """
    
    liz = lizard.analyze_file(file_path)
    functions_info = []
    for liz_elem in liz.function_list:
        functions_info.append([liz_elem.long_name, liz_elem.start_line, liz_elem.end_line])
    
    return functions_info


def extract_predicted_method_from_output(model, model_output, file_extention, gpt_flag = False, hat = '', method_index = 0):
    """
    Description of the Function:
        returns all the java methods found in the output of a model.

    Parameters:
        model_output (str): output of the model in which to search for java methods;
        hat (str): optional "hat" to prepend to the model output;
        method_index (int or array): index of the method that wants to be extracted (ie, if [0,1] the first and the second method are extracted).

    Returns:
        list: list of the java methods extracted. If no java methods are present in the model output, an empty list is returned.
   """

    methods_found = []
    
    # if the call to the model failed
    if 'FAILED.' in model_output or len(model_output) == 0:
        return methods_found
    
    model_output = model_output.replace("'", "<SINGLE_QUOTE>")
    model_output = model_output.replace("\"", "<DOUBLE_QUOTE>")

    # write the model output to a .java/.py file
    prediction_file = '../data/temp/{0}{1}.{2}'.format(model.split('/')[-1], datetime.now(), file_extention)
    with open(prediction_file, 'w') as w:
        w.write(hat)
        w.write('\n') if model_output[0] != '\n' else w.write('') # models are expected to continue writing the function starting from the signature, so the first char must be a newline
        w.write('    ') if file_extention == 'py' and not gpt_flag else w.write('') # if the target is a python method the first line after the signature is indentated
        
        # special attention to methods with annotation
        was_hat = True if len(hat) > 0 and hat[0] == '@' else False
        for line in model_output.split('\n'):
            w.write('{}\n'.format(line.strip(' '))) if was_hat else w.write(f'{line}\n') # the line following the one of the annotation is stripped
            was_hat = True if len(line) > 0 and line [0] == '@' else False

    w.close()

    listMethods = search_function(prediction_file) # run the parser on the written file
    
    with open(prediction_file, 'r') as r:
        # loop over the methods that were found by the parser
        for method in listMethods:
            methodStartLine, methodEndLine = int(method[1]) - 1, int(method[2]) - 1
            method_to_append = ''
            r.seek(0)
            for i, line in enumerate(r):
                line = line.replace("<SINGLE_QUOTE>", "'")
                line = line.replace("<DOUBLE_QUOTE>", "\"")
                
                # check the line before the start line of the method
                if i == methodStartLine - 1 and len(line.strip('\n\t \"\'')) > 0 and line.strip('\n\t \"\'')[0] == '@': #if the line before the method signature there is an annotation
                    annotation = line.strip('\n')
                    method_to_append += f'{annotation}\n' if file_extention == 'py' else f'{annotation} '
                    
                # append the lines of the method
                if i >= methodStartLine and i <= methodEndLine:
                    method_to_append += f'{line}'
            
            methods_found.append(method_to_append.strip('\n\t \"\''))
    r.close()
        
    methods_found = np.array(methods_found)

    try:
        # this covers the case in which a function is defined inside another function and we want to extract the external one
        if int(listMethods[method_index + 1][1] - 1) < int(listMethods[method_index][1] - 1) and int(listMethods[method_index + 1][2] - 1) > int(listMethods[method_index][2] - 1):
            return methods_found[method_index + 1]
        return methods_found[method_index]
    except IndexError:
        try:
            #goes here in the case where the method_index+1 is out of bounds but method_index is not
            return methods_found[method_index]
        except IndexError:
            return methods_found

def call_model(model, prompt, max_new_tokens, task):
    """
    Description of the Function:
        given a model and a prompt calls the model through the API or Inference Endpoint and returns the output.

    Parameters:
        model (str): name of the model to query;
        prompt (str): model input;
        max_new_tokens (int): maximum number of tokens generated by the model (input tokens excluded);
        task (str): just a string which explicits the task ("PREDICTION", "RANKING", "REFINEMENT") in case of failure.

    Returns:
        str: output of the model.
   """
    
    if IEP_MODELS[model] == "API":
        try:
            client = InferenceClient(model = model, token = CONSTANTS['Hugging Face API Token'])
            predicted_output = client.text_generation(prompt = prompt, max_new_tokens = max_new_tokens)
        except:
            print(Fore.RED + f'\n{model}: API {task} FAILED.\n' + Fore.BLACK)
            predicted_output = f'{model}: API {task} FAILED.'
    
    else:
        try:
            predicted_output = query_inference_endpoint(model = model, prompt = prompt, max_new_tokens = max_new_tokens)
        except:
            print(Fore.RED + f'\n{model}: IEP {task} FAILED.\n' + Fore.BLACK)
            predicted_output = f'{model}: IEP {task} FAILED.'
    
    return predicted_output.strip('\n\t ')


def ask4prediction(model, targetInstance, file_extention):
    """
    Description of the Function:
        given a model and java method of the CoderEval dataset, it returns the prediction of the model for the given target.

    Parameters:
        model (str): name of the model to query;
        targetInstance (dict): instance of the CoderEval dataset. Each instance is a dictionary containing information about the java method.

    Returns:
        str: implementation of the method predicted by the model.
   """
    
    lexer = JavaLexer()
    docstring = targetInstance['human_label']
    targetMethod = targetInstance['code']
    target_num_tokens = len(list(lexer.get_tokens(targetMethod)))
    first_line = targetMethod.split('\n')[0].strip()
    
    # adjusting the signature
    if file_extention == 'py' and len(first_line) > 0 and first_line[0] == '@':
        signature = '\n'.join([line.strip() for line in targetMethod.split('\n')[:2]])
    elif file_extention == 'py' and len(first_line) > 0 and first_line.split(' ')[0] == 'import':
        signature = targetMethod.split('\n')[1].strip()
    else:
        signature = targetMethod.split('\n')[0].strip()
    
    prompt = f'{docstring}\n{signature}'

    predicted_output = call_model(model = model, prompt = prompt, max_new_tokens = 2 * target_num_tokens, task = 'PREDICTION')
    predicted_method = extract_predicted_method_from_output(model = model, model_output = predicted_output, file_extention = file_extention, hat = signature)
    
    return predicted_method if predicted_method != [] else '' #if no methods are found in the output of the model, predicted_method will be en empty list
